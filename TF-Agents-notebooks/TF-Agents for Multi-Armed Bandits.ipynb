{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.drivers import driver\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any leftover state from previous colabs run.\n",
    "# (This is not necessary for normal programs.)\n",
    "tf.compat.v1.enable_resource_variables()\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "nest = tf.compat.v2.nest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi-Armed Bandit problem (MAB) is a special case of Reinforcement Learning: an agent collects rewards in an environment by taking some actions after observing some state of the environment. The main difference between general RL and MAB is that in MAB, we assume that the action taken by the agent does not influence the next state of the environment. Therefore, agents do not model state transitions, credit rewards to past actions, or \"plan ahead\" to get to reward-rich states.\n",
    "\n",
    "As in other RL domains, the goal of a MAB agent is to find a policy that collects as much reward as possible. It would be a mistake, however, to always try to exploit the action that promises the highest reward, because then there is a chance that we miss out on better actions if we do not explore enough. This is the main problem to be solved in (MAB), often called the exploration-exploitation dilemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In TF-Agents, the environment class serves the role of giving information on the current state (this is called observation or context), receiving an action as input, performing a state transition, and outputting a reward. This class also takes care of resetting when an episode ends, so that a new episode can start. This is realized by calling a reset function when a state is labelled as \"last\" of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
    "    def __init__(self, observation_spec, action_spec):\n",
    "        self._observation_spec = observation_spec\n",
    "        self._action_spec = action_spec\n",
    "        super(BanditPyEnvironment, self).__init__()\n",
    "    \n",
    "    # Helper functions.\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _empty_observation(self):\n",
    "        return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
    "                                     self.observation_spec())\n",
    "    # These two functions below should not be overridden by subclasses.\n",
    "    def _reset(self):\n",
    "        return ts.restart(self._observe(), batch_size=self.batch_size)\n",
    "    \n",
    "    def _step(self, action):\n",
    "        reward = self._apply_action(action)\n",
    "        return ts.termination(self._observe(), reward)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _observe(self):\n",
    "        \"\"\"Returns an observation.\"\"\"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _apply_action(self, action):\n",
    "        \"\"\"Applies `action` to the Environment and returns the corresponding reward.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePyEnvironment(BanditPyEnvironment):\n",
    "    def __init__(self):\n",
    "        action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "        observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
    "        super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
    "    \n",
    "    def _observe(self):\n",
    "        self._observation = np.random.randint(-2, 3, (1,))\n",
    "        return self._observation\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        return action * self._observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class gives a very simple environment for which the observation is a random integer between -2 and 2, there are 3 possible actions (0, 1, 2), and the reward is the product of the action and the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: -1\n",
      "action: 2\n",
      "reward: -2.000000\n"
     ]
    }
   ],
   "source": [
    "environment = SimplePyEnvironment()\n",
    "observation = environment.reset().observation\n",
    "print(\"observation: %d\" % observation)\n",
    "\n",
    "action = 2 #@param\n",
    "\n",
    "print(\"action: %d\" % action)\n",
    "reward = environment.step(action).reward\n",
    "print(\"reward: %f\" % reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define a bandit environment by subclassing BanditTFEnvironment, or, similarly to RL environments, one can define a BanditPyEnvironment and wrap it with TFPyEnvironment. For the sake of simplicity, we go with the latter option in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy in a bandit problem works the same way as in an RL problem: it provides an action (or a distribution of actions), given an observation as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with environments, there are two ways to construct a policy: One can create a PyPolicy and wrap it with TFPyPolicy, or directly create a TFPolicy. Here we elect to go with the direct method.\n",
    "\n",
    "Since this example is quite simple, we can define the optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignPolicy(tf_policy.Base):\n",
    "    def __init__(self):\n",
    "        observation_spec = tensor_spec.BoundedTensorSpec(\n",
    "            shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
    "        time_step_spec = ts.time_step_spec(observation_spec)\n",
    "\n",
    "        action_spec = tensor_spec.BoundedTensorSpec(\n",
    "            shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
    "\n",
    "        super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
    "                                         action_spec=action_spec)\n",
    "    \n",
    "    def _distribution(self, time_step):\n",
    "        pass\n",
    "    \n",
    "    def _variables(self):\n",
    "        return ()\n",
    "    \n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
    "        action = observation_sign + 1\n",
    "        return policy_step.PolicyStep(action, policy_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can request an observation from the environment, call the policy to choose an action, then the environment will output the reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      "tf.Tensor([[2]], shape=(1, 1), dtype=int64)\n",
      "Action:\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "Reward:\n",
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sign_policy = SignPolicy()\n",
    "\n",
    "current_time_step = tf_environment.reset()\n",
    "print('Observation:')\n",
    "print (current_time_step.observation)\n",
    "action = sign_policy.action(current_time_step).action\n",
    "print('Action:')\n",
    "print (action)\n",
    "reward = tf_environment.step(action).reward\n",
    "print('Reward:')\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way bandit environments are implemented ensures that every time we take a step, we not only receive the reward for the action we took, but also the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: \n",
      "tf.Tensor([[-1.]], shape=(1, 1), dtype=float32)\n",
      "Next observation:\n",
      "tf.Tensor([[-2]], shape=(1, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "step = tf_environment.reset()\n",
    "action = 1\n",
    "next_step = tf_environment.step(action)\n",
    "reward = next_step.reward\n",
    "next_observation = next_step.observation\n",
    "print(\"Reward: \")\n",
    "print(reward)\n",
    "print(\"Next observation:\")\n",
    "print(next_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API for bandit agents does not differ from that of RL agents: the agent just needs to implement the _initialize and _train methods, and define a policy and a collect_policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward sign:\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
    "    def __init__(self):\n",
    "        action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "        observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
    "\n",
    "        # Flipping the sign with probability 1/2.\n",
    "        self._reward_sign = 2 * np.random.randint(2) - 1\n",
    "        print(\"reward sign:\")\n",
    "        print(self._reward_sign)\n",
    "\n",
    "        super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
    "    \n",
    "    def _observe(self):\n",
    "        self._observation = np.random.randint(-2, 3, (1,))\n",
    "        return self._observation\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        return self._reward_sign * action * self._observation[0]\n",
    "\n",
    "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complicated Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complicated environment calls for a more complicated policy. We need a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:\n",
    "\n",
    "- The agent has not detected know yet which version of the environment is running.\n",
    "- The agent detected that the original version of the environment is running.\n",
    "- The agent detected that the flipped version of the environment is running.\n",
    "\n",
    "We define a tf_variable named _situation to store this information encoded as values in [0, 2], then make the policy behave accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoWaySignPolicy(tf_policy.Base):\n",
    "    def __init__(self, situation):\n",
    "        observation_spec = tensor_spec.BoundedTensorSpec(\n",
    "            shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
    "        action_spec = tensor_spec.BoundedTensorSpec(\n",
    "            shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
    "        time_step_spec = ts.time_step_spec(observation_spec)\n",
    "        self._situation = situation\n",
    "        super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
    "                                               action_spec=action_spec)\n",
    "    \n",
    "    def _distribution(self, time_step):\n",
    "        pass\n",
    "    \n",
    "    def _variables(self):\n",
    "        return [self._situation]\n",
    "    \n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
    "        def case_unknown_fn():\n",
    "            return tf.constant(1, shape=(1,))\n",
    "\n",
    "        def case_normal_fn():\n",
    "            return tf.constant(sign + 1, shape=(1,))\n",
    "        def case_flipped_fn():\n",
    "            return tf.constant(1 - sign, shape=(1,))\n",
    "\n",
    "        cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
    "                 (tf.equal(self._situation, 1), case_normal_fn),\n",
    "                 (tf.equal(self._situation, 2), case_flipped_fn)]\n",
    "        action = tf.case(cases, exclusive=True)\n",
    "\n",
    "        return policy_step.PolicyStep(action, policy_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define the agent that detects the sign of the environment and sets the policy appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignAgent(tf_agent.TFAgent):\n",
    "    def __init__(self):\n",
    "        self._situation = tf.compat.v2.Variable(0, dtype=tf.int32)\n",
    "        policy = TwoWaySignPolicy(self._situation)\n",
    "        time_step_spec = policy.time_step_spec\n",
    "        action_spec = policy.action_spec\n",
    "        super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
    "                                        action_spec=action_spec,\n",
    "                                        policy=policy,\n",
    "                                        collect_policy=policy,\n",
    "                                        train_sequence_length=None)\n",
    "    def _initialize(self):\n",
    "        return tf.compat.v1.variables_initializer(self.variables)\n",
    "    \n",
    "    def _train(self, experience, weights=None):\n",
    "        observation = experience.observation\n",
    "        action = experience.action\n",
    "        reward = experience.reward\n",
    "\n",
    "        # We only need to change the value of the situation variable if it is\n",
    "        # unknown (0) right now, and we can infer the situation only if the\n",
    "        # observation is not 0.\n",
    "        needs_action = tf.logical_and(tf.equal(self._situation, 0), tf.not_equal(reward, 0))\n",
    "        \n",
    "        def new_situation_fn():\n",
    "            \"\"\"This returns either 1 or 2, depending on the signs.\"\"\"\n",
    "            return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
    "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
    "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
    "    \n",
    "        new_situation = tf.cond(needs_action, new_situation_fn, lambda: self._situation)\n",
    "        tf.compat.v1.assign(self._situation, new_situation)\n",
    "\n",
    "        return tf_agent.LossInfo((), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_agent = SignAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the agent defines the policy, and the variable situation is shared by the agent and the policy.\n",
    "\n",
    "Also, the parameter experience of the _train function is a trajectory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TF-Agents, trajectories are named tuples that contain samples from previous steps taken. These samples are then used by the agent to train and update the policy. In RL, trajectories must contain information about the current state, the next state, and whether the current episode has ended. Since in the Bandit world we do not need these things, we set up a helper function to create a trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add another dimension here because the agent expects the\n",
    "# trajectory of shape [batch_size, time, ...], but in this tutorial we assume\n",
    "# that both batch size and time are 1. Hence all the expand_dims.\n",
    "\n",
    "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "    return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
    "                               action=tf.expand_dims(action_step.action, 0),\n",
    "                               policy_info=action_step.info,\n",
    "                               reward=tf.expand_dims(final_step.reward, 0),\n",
    "                               discount=tf.expand_dims(final_step.discount, 0),\n",
    "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
    "                               next_step_type=tf.expand_dims(final_step.step_type, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-2.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n",
      "Trajectory(step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 1, 1), dtype=int64, numpy=array([[[0]]])>, action=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "step = two_way_tf_environment.reset()\n",
    "for _ in range(10):\n",
    "    action_step = sign_agent.collect_policy.action(step)\n",
    "    next_step = two_way_tf_environment.step(action_step.action)\n",
    "    experience = trajectory_for_bandit(step, action_step, next_step)\n",
    "    print(experience)\n",
    "#     sign_agent.train(experience)\n",
    "#     step = next_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Real Contextual Bandit Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\n",
    "from tf_agents.bandits.metrics import tf_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 # @param\n",
    "arm0_param = [-3, 0, 1, -2] # @param\n",
    "arm1_param = [1, -2, 3, 0] # @param\n",
    "arm2_param = [0, 0, 1, 1] # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_sampling_fn(batch_size):\n",
    "    def _context_sampling_fn():\n",
    "        return np.random.randint(-10, 10, [batch_size, 4]).astype(np.float32)\n",
    "    return _context_sampling_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNormalReward(object):\n",
    "    def __init__(self, theta, sigma):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "    def __call__(self, x):\n",
    "        mu = np.dot(x, self.theta)\n",
    "        return np.random.normal(mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm0_reward_fn = LinearNormalReward(arm0_param, 1)\n",
    "arm1_reward_fn = LinearNormalReward(arm1_param, 1)\n",
    "arm2_reward_fn = LinearNormalReward(arm2_param, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = tf_py_environment.TFPyEnvironment(\n",
    "    sspe.StationaryStochasticPyEnvironment(\n",
    "        context_sampling_fn(batch_size),\n",
    "        [arm0_reward_fn, arm1_reward_fn, arm2_reward_fn],\n",
    "        batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec = tensor_spec.TensorSpec([4], tf.float32)\n",
    "time_step_spec = ts.time_step_spec(observation_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    dtype=tf.int32, shape=(), minimum=0, maximum=2)\n",
    "\n",
    "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
    "                                     action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandits' most important metric is regret, calculated as the difference between the reward collected by the agent and the expected reward of an oracle policy that has access to the reward functions of the environment. The RegretMetric thus needs a baseline_reward_fn function that calculates the best achievable expected reward given an observation. For our example, we need to take the maximum of the no-noise equivalents of the reward functions that we already defined for the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_reward(observation):\n",
    "    expected_reward_for_arms = [\n",
    "      tf.linalg.matvec(observation, tf.cast(arm0_param, dtype=tf.float32)),\n",
    "      tf.linalg.matvec(observation, tf.cast(arm1_param, dtype=tf.float32)),\n",
    "      tf.linalg.matvec(observation, tf.cast(arm2_param, dtype=tf.float32))]\n",
    "    optimal_action_reward = tf.reduce_max(expected_reward_for_arms, axis=0)\n",
    "    return optimal_action_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_metric = tf_metrics.RegretMetric(compute_optimal_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put together all the components that we introduced above: the environment, the policy, and the agent. We run the policy on the environment and output training data with the help of a driver, and train the agent on the data.\n",
    "\n",
    "Note that there are two parameters that together specify the number of steps taken. num_iterations specifies how many times we run the trainer loop, while the driver will take steps_per_loop steps per iteration. The main reason behind keeping both of these parameters is that some operations are done per iteration, while some are done by the driver in every step. For example, the agent's train function is only called once per iteration. The trade-off here is that if we train more often then our policy is \"fresher\", on the other hand, training in bigger batches might be more time efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of Iterations')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_iterations = 90 # @param\n",
    "steps_per_loop = 1 # @param\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=steps_per_loop)\n",
    "\n",
    "observers = [replay_buffer.add_batch, regret_metric]\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=environment,\n",
    "    policy=agent.collect_policy,\n",
    "    num_steps=steps_per_loop * batch_size,\n",
    "    observers=observers)\n",
    "\n",
    "regret_values = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    driver.run()\n",
    "    loss_info = agent.train(replay_buffer.gather_all())\n",
    "    replay_buffer.clear()\n",
    "    regret_values.append(regret_metric.result())\n",
    "\n",
    "plt.plot(regret_values)\n",
    "plt.ylabel('Average Regret')\n",
    "plt.xlabel('Number of Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
