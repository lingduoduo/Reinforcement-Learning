{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already satisfied: tf-agents in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.3.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (1.18.1)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.1.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.6.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: gast>=0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (0.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (1.2.2)\n",
      "Requirement already satisfied: decorator in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (4.4.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already satisfied: gym==0.10.11 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.10.11)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym==0.10.11) (1.18.1)\n",
      "Requirement already satisfied: requests>=2.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym==0.10.11) (2.22.0)\n",
      "Requirement already satisfied: scipy in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym==0.10.11) (1.4.1)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym==0.10.11) (1.3.2)\n",
      "Requirement already satisfied: six in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym==0.10.11) (1.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.11) (2.8)\n",
      "Requirement already satisfied: future in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from pyglet>=1.2.0->gym==0.10.11) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "# Note: If you haven't installed tf-agents or gym yet, run:\n",
    "!pip install tf-agents\n",
    "!pip install 'gym==0.10.11'\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Reinforcement Learning (RL) is to design agents that learn by interacting with an environment. In the standard RL setting, the agent receives an observation at every time step and chooses an action. The action is applied to the environment and the environment returns a reward and a new observation. The agent trains a policy to choose actions to maximize the sum of rewards, also known as return.\n",
    "\n",
    "In TF-Agents, environments can be implemented either in Python or TensorFlow. Python environments are usually easier to implement, understand, and debug, but TensorFlow environments are more efficient and allow natural parallelization. The most common workflow is to implement an environment in Python and use one of our wrappers to automatically convert it into TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python environments have a step(action) -> next_time_step method that applies an action to the environment, and returns the following information about the next step:\n",
    "\n",
    "- observation: This is the part of the environment state that the agent can observe to choose its actions at the next step.\n",
    "- reward: The agent is learning to maximize the sum of these rewards across multiple steps.\n",
    "- step_type: Interactions with the environment are usually part of a sequence/episode. e.g. multiple moves in a game of chess. step_type can be either FIRST, MID or LAST to indicate whether this time step is the first, intermediate or last step in a sequence.\n",
    "- discount: This is a float representing how much to weight the reward at the next time step relative to the reward at the current time step.\n",
    "These are grouped into a named tuple TimeStep(step_type, reward, discount, observation).\n",
    "\n",
    "The interface that all python environments must implement is in environments/py_environment.PyEnvironment. The main methods are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyEnvironment(object):\n",
    "\n",
    "    def reset(self):\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        if self._current_time_step is None: return self.reset()\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "    \n",
    "    def current_time_step(self):\n",
    "        return self._current_time_step\n",
    "\n",
    "    def time_step_spec(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def observation_spec(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def action_spec(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _reset(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _step(self, action):\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Standard Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "time_step_spec.observation: BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
      "time_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', environment.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.00804041,  0.02778698, -0.03214975, -0.04846389], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.00859615,  0.22335483, -0.03311903, -0.35111448], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01306325,  0.41893172, -0.04014132, -0.65405416], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.02144188,  0.614589  , -0.05322241, -0.959102  ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03373366,  0.81038445, -0.07240444, -1.2680193 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.04994135,  1.0063527 , -0.09776483, -1.5824698 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.0700684 ,  1.2024926 , -0.12941423, -1.9039719 ], dtype=float32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.09411825,  1.3987542 , -0.16749367, -2.2338443 ], dtype=float32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32), observation=array([ 0.12209334,  1.5950229 , -0.21217056, -2.5731404 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "while not time_step.is_last():\n",
    "    time_step = environment.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to train an agent to play the following (Black Jack inspired) card game:\n",
    "\n",
    "- The game is played using an infinite deck of cards numbered 1...10.\n",
    "- At every turn the agent can do 2 things: get a new random card, or stop the current round.\n",
    "- The goal is to get the sum of your cards as close to 21 as possible at the end of the round, without going over.\n",
    "\n",
    "An environment that represents the game could look like this:\n",
    "\n",
    "- Actions: We have 2 actions. Action 0: get a new card, and Action 1: terminate the current round.\n",
    "- Observations: Sum of the cards in the current round.\n",
    "- Reward: The objective is to get as close to 21 as possible without going over, so we can achieve this using the following reward at the end of the round: sum_of_cards - 21 if sum_of_cards <= 21, else -21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        # Make sure episodes don't go on forever.\n",
    "        if action == 1:\n",
    "            self._episode_ended = True\n",
    "        elif action == 0:\n",
    "            new_card = np.random.randint(1, 11)\n",
    "            self._state += new_card\n",
    "        else:\n",
    "            raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "        if self._episode_ended or self._state >= 21:\n",
    "            reward = self._state - 21 if self._state <= 21 else -21\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([5], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([8], dtype=int32))\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([9], dtype=int32))\n",
      "TimeStep(step_type=array(2, dtype=int32), reward=array(-12., dtype=float32), discount=array(0., dtype=float32), observation=array([9], dtype=int32))\n",
      "Final Reward =  -12.0\n"
     ]
    }
   ],
   "source": [
    "get_new_card_action = 0\n",
    "end_round_action = 1\n",
    "\n",
    "environment = CardGameEnv()\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(3):\n",
    "    time_step = environment.step(get_new_card_action)\n",
    "    print(time_step)\n",
    "    cumulative_reward += time_step.reward\n",
    "\n",
    "time_step = environment.step(end_round_action)\n",
    "print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec: BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-2.0, maximum=2.0)\n",
      "Discretized Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=4)\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('Pendulum-v0')\n",
    "print('Action Spec:', env.action_spec())\n",
    "\n",
    "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
    "print('Discretized Action Spec:', discrete_action_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface for TF environments is defined in environments/tf_environment.TFEnvironment and looks very similar to the Python environments. TF Environments differ from python envs in a couple of ways:\n",
    "\n",
    "- They generate tensor objects instead of arrays\n",
    "- TF environments add a batch dimension to the tensors generated when compared to the specs.\n",
    "\n",
    "Converting the python environments into TFEnvs allows tensorflow to parellalize operations. For example, one could define a collect_experience_op that collects data from the environment and adds to a replay_buffer, and a train_op that reads from the replay_buffer and trains the agent, and run them in parallel naturally in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFEnvironment(object):\n",
    "\n",
    "    def time_step_spec(self):\n",
    "        pass\n",
    "\n",
    "    def observation_spec(self):\n",
    "        pass\n",
    "\n",
    "    def action_spec(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        return self._reset()\n",
    "\n",
    "    def current_time_step(self):\n",
    "        return self._current_time_step()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self._step(action)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _reset(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _current_time_step(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _step(self, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping a Python Environment in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)))\n",
      "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(step_type=array([0], dtype=int32), reward=array([0.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.03512056, -0.00113605, -0.01196977,  0.01381821]],\n",
      "      dtype=float32)), array([0], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.03509784, -0.1960843 , -0.01169341,  0.3027006 ]],\n",
      "      dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.03509784, -0.1960843 , -0.01169341,  0.3027006 ]],\n",
      "      dtype=float32)), array([1], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.03117616, -0.00079767, -0.0056394 ,  0.00635291]],\n",
      "      dtype=float32))]\n",
      "[TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.03117616, -0.00079767, -0.0056394 ,  0.00635291]],\n",
      "      dtype=float32)), array([0], dtype=int32), TimeStep(step_type=array([1], dtype=int32), reward=array([1.], dtype=float32), discount=array([1.], dtype=float32), observation=array([[ 0.0311602 , -0.19583829, -0.00551234,  0.29725122]],\n",
      "      dtype=float32))]\n",
      "Total reward: [3.]\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# reset() creates the initial time_step after resetting the environment.\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "for i in range(num_steps):\n",
    "    action = tf.constant([i % 2])\n",
    "    # applies the action and returns the new TimeStep.\n",
    "    next_time_step = tf_env.step(action)\n",
    "    transitions.append([time_step, action, next_time_step])\n",
    "    reward += next_time_step.reward\n",
    "    time_step = next_time_step\n",
    "\n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_episodes: 5 num_steps: 104\n",
      "avg_length 20.8 avg_reward: 20.8\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not time_step.is_last():\n",
    "        action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "        time_step = tf_env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += time_step.reward.numpy()\n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
