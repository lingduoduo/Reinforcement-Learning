{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already up-to-date: tensorflow-probability in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=1.2.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (1.18.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already satisfied: tf-agents in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.3.0)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.1.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.6.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (1.18.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (1.2.2)\n",
      "Requirement already satisfied: gast>=0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (0.3.3)\n",
      "Requirement already satisfied: decorator in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (4.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Note: If you haven't installed tf-agents yet, run:\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "!pip install --upgrade tensorflow-probability\n",
    "!pip install tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Reinforcement Learning terminology, policies map an observation from the environment to an action or a distribution over actions. In TF-Agents, observations from the environment are contained in a named tuple TimeStep('step_type', 'discount', 'reward', 'observation'), and policies map timesteps to actions or distributions over actions. Most policies use timestep.observation, some policies use timestep.step_type (e.g. to reset the state at the beginning of an episode in stateful policies), but timestep.discount and timestep.reward are usually ignored.\n",
    "\n",
    "Policies are related to other components in TF-Agents in the following way. Most policies have a neural network to compute actions and/or distributions over actions from TimeSteps. Agents can contain one or more policies for different purposes, e.g. a main policy that is being trained for deployment, and a noisy policy for data collection. Policies can be saved/restored, and can be used indepedently of the agent for data collection, evaluation etc.\n",
    "\n",
    "Some policies are easier to write in Tensorflow (e.g. those with a neural network), whereas others are easier to write in Python (e.g. following a script of actions). So in TF agents, we allow both Python and Tensorflow policies. Morever, policies written in TensorFlow might have to be used in a Python environment, or vice versa, e.g. a TensorFlow policy is used for training but later deployed in a production python environment. To make this easier, we provide wrappers for converting between python and TensorFlow policies.\n",
    "\n",
    "Another interesting class of policies are policy wrappers, which modify a given policy in a certain way, e.g. add a particular type of noise, make a greedy or epsilon-greedy version of a stochastic policy, randomly mix multiple policies etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(object):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, time_step_spec, action_spec, policy_state_spec=()):\n",
    "        self._time_step_spec = time_step_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._policy_state_spec = policy_state_spec\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reset(self, policy_state=()):\n",
    "    # return initial_policy_state.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def action(self, time_step, policy_state=()):\n",
    "    # return a PolicyStep(action, state, info) named tuple.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def distribution(self, time_step, policy_state=()):\n",
    "    # Not implemented in python, only for TF policies.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, policy):\n",
    "    # update self to be similar to the input `policy`.\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def copy(self):\n",
    "    # return a copy of self.\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def time_step_spec(self):\n",
    "        return self._time_step_spec\n",
    "\n",
    "    @property\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    @property\n",
    "    def policy_state_spec(self):\n",
    "        return self._policy_state_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important method is action(time_step) which maps a time_step containing an observation from the environment to a PolicyStep named tuple containing the following attributes:\n",
    "\n",
    "- action: The action to be applied to the environment.\n",
    "- state: The state of the policy (e.g. RNN state) to be fed into the next call to action.\n",
    "- info: Optional side information such as action log probabilities.\n",
    "\n",
    "The time_step_spec and action_spec are specifications for the input time step and the output action. Policies also have a reset function which is typically used for resetting the state in stateful policies. The copy function returns a copy of self and the update(new_policy) function updates self towards new_policy.\n",
    "\n",
    "Now, let us look at a couple of examples of python policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyStep(action=array([ 10, -10], dtype=int32), state=(), info=())\n",
      "PolicyStep(action=array([ 6, 10], dtype=int32), state=(), info=())\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None,\n",
    "    action_spec=action_spec)\n",
    "time_step = None\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing scripted policy...\n",
      "PolicyStep(action=array([5, 2], dtype=int32), state=[0, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 1], info=())\n",
      "PolicyStep(action=array([1, 2], dtype=int32), state=[2, 2], info=())\n",
      "Resetting my_scripted_py_policy...\n"
     ]
    }
   ],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "action_script = [(1, np.array([5, 2], dtype=np.int32)), \n",
    "                 (0, np.array([0, 0], dtype=np.int32)), # Setting `num_repeates` to 0 will skip this action.\n",
    "                 (2, np.array([1, 2], dtype=np.int32)), \n",
    "                 (1, np.array([3, 4], dtype=np.int32))]\n",
    "\n",
    "my_scripted_py_policy = scripted_py_policy.ScriptedPyPolicy(time_step_spec=None, action_spec=action_spec, action_script=action_script)\n",
    "\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "time_step = None\n",
    "print('Executing scripted policy...')\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)\n",
    "action_step= my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "action_step = my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "print('Resetting my_scripted_py_policy...')\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "action_step = my_scripted_py_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Random TF Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor([-0.01249933  1.4199572 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    (2,), tf.float32, minimum=-1, maximum=3)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec)\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "time_step = ts.restart(observation)\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Actor Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An actor policy can be created using either a network that maps time_steps to actions or a network that maps time_steps to distributions over actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "        super(ActionNet, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name='ActionNet')\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._layers = [\n",
    "            tf.keras.layers.Dense(\n",
    "                action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "        ]\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        del step_type\n",
    "\n",
    "        output = tf.cast(observations, dtype=tf.float32)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "        # Scale and shift actions to the correct range if necessary.\n",
    "        return actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-0.45615217  0.7324672  -0.1420335 ]\n",
      " [-0.45615217  0.7324672  -0.1420335 ]], shape=(2, 3), dtype=float32)\n",
      "Action distribution:\n",
      "tfp.distributions.Deterministic(\"Deterministic\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "observations = tf.ones([2] + time_step_spec.observation.shape.as_list())\n",
    "\n",
    "time_step = ts.restart(observations, batch_size)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[-1.          0.47452495 -0.8223118 ]\n",
      " [-0.7682057  -0.01630288  0.337681  ]], shape=(2, 3), dtype=float32)\n",
      "Action distribution:\n",
      "tfp.distributions.Normal(\"ActionNet_Normal\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class ActionDistributionNet(ActionNet):\n",
    "\n",
    "    def call(self, observations, step_type, network_state):\n",
    "        action_means, network_state = super(ActionDistributionNet, self).call(\n",
    "        observations, step_type, network_state)\n",
    "\n",
    "        action_std = tf.ones_like(action_means)\n",
    "        return tfp.distributions.Normal(action_means, action_std), network_state\n",
    "\n",
    "\n",
    "action_distribution_net = ActionDistributionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_distribution_net)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]], shape=(2, 1), dtype=int32)\n",
      "Action distribution:\n",
      "tfp.distributions.ShiftedCategorical(\"ShiftedCategorical\", batch_shape=[2, 1], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((1,),\n",
    "                                            tf.int32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "\n",
    "class QNetwork(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, action_spec, num_actions=num_actions, name=None):\n",
    "        super(QNetwork, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=(),\n",
    "        name=name)\n",
    "        self._layers.append(tf.keras.layers.Dense(num_actions))\n",
    "    \n",
    "    def call(self, inputs, step_type=None, network_state=()):\n",
    "        del step_type\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        return inputs, network_state\n",
    "\n",
    "batch_size = 2\n",
    "observation = tf.ones([batch_size] + time_step_spec.observation.shape.as_list())\n",
    "time_steps = ts.restart(observation, batch_size=batch_size)\n",
    "\n",
    "my_q_network = QNetwork(\n",
    "    input_tensor_spec=input_tensor_spec,\n",
    "    action_spec=action_spec)\n",
    "my_q_policy = q_policy.QPolicy(\n",
    "    time_step_spec, action_spec, q_network=my_q_network)\n",
    "action_step = my_q_policy.action(time_steps)\n",
    "distribution_step = my_q_policy.distribution(time_steps)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]], shape=(2, 1), dtype=int32)\n",
      "Action distribution:\n",
      "tfp.distributions.DeterministicWithLogProb(\"Deterministic\", batch_shape=[2, 1], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "my_greedy_policy = greedy_policy.GreedyPolicy(my_q_policy)\n",
    "\n",
    "action_step = my_greedy_policy.action(time_steps)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_greedy_policy.distribution(time_steps)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
