{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.16 (default, Apr 12 2019, 15:32:40) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.3)]\n",
      "1.9.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "# Examine software versions\n",
    "print(__import__('sys').version)\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotify_tensorflow.dataset import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'paradox-mo'  #@param {type:\"string\"}\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gsutil ls gs://mo_ml/lingh/tfrecords/pdx_mo.Push.BaseInputDataV1.train/2019-08-21/20190827T173336.732193-c52ea29f77ca\n",
    "\n",
    "gsutil ls gs://mo_ml/lingh/tfrecords/pdx_mo.Push.BaseInputDataV1.evaluation/2019-08-21/20190827T173336.731882-f43a57c51327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east1' \n",
    "BUCKET_NAME = 'mo_ml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mo_ml/...\n",
      "ServiceException: 409 Bucket mo_ml already exists.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scio Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ghe.spotify.net/paradox/messaging-optimization/blob/master/messaging-optimization-pipeline/tf-supervised/src/main/python/trainers/features/FeatureTask.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [ml_engine/local_python].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set ml_engine/local_python $(which python3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/spotify/spotify-tensorflow/blob/master/examples/example_tf_training_xgb/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named examples.examples_utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-500fb843b7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspotify_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named examples.examples_utils"
     ]
    }
   ],
   "source": [
    "from spotify_tensorflow.examples.examples_utils import get_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = get_data_dir(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotify_tensorflow as sptf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sptf.tf_schema_utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud ai-platform local train \\\n",
    "  --package-path trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  --job-dir local-training-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Fetch Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotify_tensorflow.dataset import Datasets\n",
    "from spotify_tensorflow.tf_schema_utils import schema_txt_file_to_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://backstage.spotify.net/docs/apocrypha-ml/apocrypha_ml_pipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = 'mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/_MANIFEST.json\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/checkpoint\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/events.out.tfevents.1567530708.Lings-MacBook-Pro.local\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/events.out.tfevents.1567530713.Lings-MacBook-Pro.local\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/graph.pbtxt\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/eval_NeuralNetEval/\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/eval_NeuralNetMetrics/\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/export/\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/training/\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/transform_fn/\r\n",
      "gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/transformed_metadata/\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls gs://$job_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = 'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transform_dir = 'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = os.path.join(tf_transform_dir, \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/training/training'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = os.path.join(tf_transform_dir, \"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/training/evaluation'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir = 'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/transformed_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/transformed_metadata/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_transform.tf_metadata import metadata_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Schema file gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/transformed_metadata/v1-json/schema.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-06f1d23ccab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow_transform/tf_metadata/metadata_io.pyc\u001b[0m in \u001b[0;36mread_metadata\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mschema_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v1-json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'schema.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Schema file {} does not exist'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Schema file gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/transformed_metadata/v1-json/schema.json does not exist"
     ]
    }
   ],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(metadata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow_transform.tf_metadata.dataset_metadata' from '/usr/local/lib/python2.7/site-packages/tensorflow_transform/tf_metadata/dataset_metadata.pyc'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_io.dataset_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Datasets.examples_via_feature_spec(os.path.join(self.training_dir, \"part-*\"),\n",
    "                                               transformed_feature_spec,\n",
    "                                               batch_size=self.batch_size,\n",
    "                                               # NOTE: we cycles through train set forever\n",
    "                                               num_epochs=None,\n",
    "                                               shuffle=True,\n",
    "                                               reader_num_threads=self.read_num_threads,\n",
    "                                               parser_num_threads=self.parse_num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Datasets.examples_via_feature_spec(os.path.join(training_dir, \"part-*\"),\n",
    "                transformed_feature_spec,\n",
    "                batch_size=25,\n",
    "                num_epochs=1000,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data set from gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ddf3115bdcc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5488\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5490\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5492\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'gs://mo_ml/lingh/push/training_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mo_ml/lingh/push/training_data/\r\n",
      "gs://mo_ml/lingh/push/training_data/part-00000-of-00003.tfrecords\r\n",
      "gs://mo_ml/lingh/push/training_data/part-00001-of-00003.tfrecords\r\n",
      "gs://mo_ml/lingh/push/training_data/part-00002-of-00003.tfrecords\r\n",
      "gs://mo_ml/lingh/push/training_data/stats.pb\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls gs://mo_ml/lingh/push/training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = os.path.join(train_data_dir, \"part-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/push/training_data/part-*'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = os.path.join(train_data_dir, \"stats.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/push/training_data/stats.pb'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Eager execution is required for a Dictionary endpoint! Add this add the begining of your main:\n\nimport tensorflow as tf\ntf.enable_eager_execution()\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-79d869d7605d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples_via_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spotify_tensorflow/dataset.pyc\u001b[0m in \u001b[0;36m_examples\u001b[0;34m(cls, file_pattern, schema_path, feature_spec, default_value, compression_type, batch_size, shuffle, num_epochs, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, reader_num_threads, parser_num_threads, sloppy_ordering, drop_final_batch)\u001b[0m\n\u001b[1;32m    348\u001b[0m                       ):\n\u001b[1;32m    349\u001b[0m             \u001b[0;31m# type: (...) -> Iterator[Dict[str, np.ndarray]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mDatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dictionary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mget_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spotify_tensorflow/dataset.pyc\u001b[0m in \u001b[0;36m_assert_eager\u001b[0;34m(endpoint)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                        \u001b[0;34m\"Add this add the begining of your main:\\n\\nimport \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                        \u001b[0;34m\"tensorflow as tf\\ntf.enable_eager_execution()\\n\\n\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                        \u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Eager execution is required for a Dictionary endpoint! Add this add the begining of your main:\n\nimport tensorflow as tf\ntf.enable_eager_execution()\n\n"
     ]
    }
   ],
   "source": [
    "training_dataset = next(Datasets.dict.examples_via_schema(train_data,schema_path,batch_size=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('gsutil -m cp gs://%s/* %s' % (train_data, tmp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecords = ['%s/*.tfrecords' % tmp_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/var/folders/xc/cd6336nn32j1x6nqkwp37hv80000gn/T/tmpOPWG4V/*.tfrecords']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords = tf.placeholder(tf.string, shape=[None])\n",
    "        iterator = get_iterator(tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-15eed125ab14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spotify_tensorflow/dataset.pyc\u001b[0m in \u001b[0;36mparse_schema\u001b[0;34m(cls, schema_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mschema_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0mSchema\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_schema_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mschema_to_feature_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spotify_tensorflow/tf_schema_utils.pyc\u001b[0m in \u001b[0;36mparse_schema_file\u001b[0;34m(schema_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDecodeError\u001b[0m: Error parsing message"
     ]
    }
   ],
   "source": [
    "feature_spec, _ = Datasets.parse_schema(schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tfrecords(data_dict):\n",
    "    # the *.featran settings file now lives inside a subfolder\n",
    "    data_dict['featran-settings'] = fix_featran_settings_path(\n",
    "        data_dict['featran-settings']\n",
    "    )\n",
    "\n",
    "    data_dict['data-train'] = download_dirs_from_gcs(data_dict['data-train'])\n",
    "    data_dict['data-dev'] = download_dirs_from_gcs(data_dict['data-dev'])\n",
    "    data_dict['data-test'] = download_dirs_from_gcs(data_dict['data-test'])\n",
    "    data_dict['featran-settings'] = download_dirs_from_gcs(\n",
    "        [data_dict['featran-settings']]\n",
    "    )\n",
    "\n",
    "    # download_dirs_from_gcs returns a list, we just want the one featran path\n",
    "    data_dict['featran-settings'] = data_dict['featran-settings'][0]\n",
    "\n",
    "    # get featran settings file in the directory\n",
    "    filenames = os.listdir(data_dict['featran-settings'])\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.featran'):\n",
    "            data_dict['featran-settings'] = os.path.join(\n",
    "                data_dict['featran-settings'],\n",
    "                filename\n",
    "            )\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Datasets.dict.examples_via_schema(train_data, schema_path, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _examples at 0x12be3c550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/push/training_data/part-*'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/push/training_data/stats.pb'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(self):\n",
    "    # A lot happens here, we:\n",
    "    # * read tf.Examples stored in tf.Records\n",
    "    # * parse Examples using features spec from normalized_feature_spec\n",
    "    # * we parallelize reading and parsing\n",
    "    # * we batch and repeat whole training dataset epoch times\n",
    "    d = Datasets.examples_via_feature_spec(os.path.join(self.training_dir, \"part-*\"),\n",
    "                                           self.transformed_feature_spec,\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           # NOTE: we cycles through train set forever\n",
    "                                           num_epochs=None,\n",
    "                                           shuffle=True,\n",
    "                                           reader_num_threads=self.read_num_threads,\n",
    "                                           parser_num_threads=self.parse_num_threads)\n",
    "    return d.map(self.split_features_label_fn, num_parallel_calls=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = get_data_dir(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0d0e8fcc91e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_metadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetadata_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformed_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata_dir' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "transformed_metadata = metadata_io.read_metadata(metadata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'TimeDomain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bc5dbb0aa7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow_data_validation/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Import stats API.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerateStatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Import validation API.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow_data_validation/api/stats_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyarrow_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnatural_language_stats_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime_stats_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtop_k_uniques_combiner_stats_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_data_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtop_k_uniques_stats_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow_data_validation/statistics/generators/time_stats_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m _UNIX_TIMES = [\n\u001b[1;32m     68\u001b[0m     _UnixTime(\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mformat_constant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeDomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNIX_SECONDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m631152000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         end=1893456000),\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'TimeDomain'"
     ]
    }
   ],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8a1e04aefbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTXT_SCHEMA_OUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'schema.pbtxt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mschema_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessageToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTXT_SCHEMA_OUTPUT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'schema' is not defined"
     ]
    }
   ],
   "source": [
    "from google.protobuf import text_format\n",
    "# Write the canonical schema to a text file\n",
    "TXT_SCHEMA_OUTPUT_PATH = metadata_dir + 'schema.pbtxt'\n",
    "\n",
    "schema_text = text_format.MessageToString(schema)\n",
    "file_io.write_string_to_file(TXT_SCHEMA_OUTPUT_PATH, schema_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Datasets.examples_via_feature_spec(os.path.join(training_dir, \"part-*\"),\n",
    "                transformed_feature_spec,\n",
    "                batch_size=25,\n",
    "                num_epochs=1000,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            d = Datasets.examples_via_feature_spec(os.path.join(training_dir, \"part-*\"),\n",
    "                                                   transformed_feature_spec,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   # cycle through the train set forever\n",
    "                                                   # For training, we don't use num_epochs as a\n",
    "                                                   # stopping criteria but use max_steps instead\n",
    "                                                   # (set in TrainSpec); we set this way as the\n",
    "                                                   # final model saving would rely on max_steps\n",
    "                                                   # always to be reached\n",
    "                                                   num_epochs=None,\n",
    "                                                   shuffle=True,\n",
    "                                                   reader_num_threads=FLAGS[\n",
    "                                                       \"reader_num_threads\"].value,\n",
    "                                                   parser_num_threads=FLAGS[\n",
    "                                                       \"parser_num_threads\"].value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(metadata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_feature_spec =  transformed_metadata.schema.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"pushLabelTable.campaign_id\"\n",
       "type: BYTES\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.channel\"\n",
       "type: BYTES\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.clicked\"\n",
       "type: INT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.date\"\n",
       "type: BYTES\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.hrs_to_click\"\n",
       "type: INT\n",
       ", name: \"pushLabelTable.message_id\"\n",
       "type: BYTES\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.optout_diff\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.optout_type\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 1\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"pushLabelTable.os_level_unsub\"\n",
       "type: INT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"pushLabelTable.time_click\"\n",
       "type: BYTES\n",
       ", name: \"pushLabelTable.time_send\"\n",
       "type: BYTES\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"pushLabelTable.user_id\"\n",
       "type: BYTES\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "  dim {\n",
       "    size: 1\n",
       "  }\n",
       "}\n",
       ", name: \"userAggTable.country_code\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 76\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.days_since_last_dau\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.days_since_last_mau\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.dsr\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.dsr_bucket\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 4\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.email_click_deliver_rate_month\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_click_deliver_rate_week\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_click_deliver_rate_yesterday\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_click_open_rate_month\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_click_open_rate_week\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_click_open_rate_yesterday\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_open_rate_month\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_open_rate_week\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.email_open_rate_yesterday\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.first_platform\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 7\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.gender\"\n",
       "type: BYTES\n",
       ", name: \"userAggTable.has_streamed_ever\"\n",
       "type: INT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.in_app_click_rate_month\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.in_app_click_rate_week\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.in_app_click_rate_yesterday\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.is_dau\"\n",
       "type: INT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.is_mau\"\n",
       "type: INT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.ms_played_day\"\n",
       "type: INT\n",
       ", name: \"userAggTable.ms_played_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_click_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_click_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_click_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_email_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_open_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_open_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_open_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_email_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_email_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_in_app_click_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_in_app_click_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_in_app_click_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_in_app_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_in_app_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_in_app_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_push_click_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_push_click_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_push_click_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_push_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_push_week\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.num_push_yesterday\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_streams_day\"\n",
       "type: INT\n",
       ", name: \"userAggTable.num_streams_month\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.primary_platform\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 10\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.product_category\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 6\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.product_type\"\n",
       "type: INT\n",
       "int_domain {\n",
       "  min: -1\n",
       "  max: 2\n",
       "  is_categorical: true\n",
       "}\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       ", name: \"userAggTable.push_click_rate_month\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.push_click_rate_week\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.push_click_rate_yesterday\"\n",
       "type: FLOAT\n",
       ", name: \"userAggTable.registered_today\"\n",
       "type: INT\n",
       ", name: \"userAggTable.reporting_age_bucket\"\n",
       "type: BYTES\n",
       ", name: \"userAggTable.user_id\"\n",
       "type: BYTES\n",
       ", name: \"weight_column\"\n",
       "type: FLOAT\n",
       "presence {\n",
       "  min_fraction: 1.0\n",
       "}\n",
       "shape {\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_feature_spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key = 'os_level_unsub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'is_match'\n",
    "        feature_names = [name for name in feature_spec if not name == target_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 12:58:18.934044 4652987840 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/spotify_tensorflow/dataset.py:210: make_batched_features_dataset (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.make_batched_features_dataset(...)`.\n",
      "W0906 12:58:19.280124 4652987840 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py:835: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0906 12:58:19.303127 4652987840 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py:212: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-88e4516c9972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/spotify_tensorflow/dataset.py\u001b[0m in \u001b[0;36mexamples_via_feature_spec\u001b[0;34m(cls, file_pattern, feature_spec, compression_type, batch_size, shuffle, num_epochs, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, reader_num_threads, parser_num_threads, sloppy_ordering, drop_final_batch)\u001b[0m\n\u001b[1;32m    166\u001b[0m                              \u001b[0mparser_num_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser_num_threads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                              \u001b[0msloppy_ordering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msloppy_ordering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                              drop_final_batch=drop_final_batch)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/spotify_tensorflow/dataset.py\u001b[0m in \u001b[0;36m_examples\u001b[0;34m(cls, file_pattern, schema_path, feature_spec, compression_type, batch_size, shuffle, num_epochs, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, reader_num_threads, parser_num_threads, sloppy_ordering, drop_final_batch)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                                 \u001b[0mparser_num_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser_num_threads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                                 \u001b[0msloppy_ordering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msloppy_ordering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                                 drop_final_batch=drop_final_batch)\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/readers.py\u001b[0m in \u001b[0;36mmake_batched_features_dataset\u001b[0;34m(file_pattern, batch_size, features, reader, label_key, reader_args, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, reader_num_threads, parser_num_threads, sloppy_ordering, drop_final_batch)\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0mprefetch_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_num_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_num_threads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m       sloppy_ordering, drop_final_batch)\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36mmake_batched_features_dataset_v1\u001b[0;34m(file_pattern, batch_size, features, reader, label_key, reader_args, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, reader_num_threads, parser_num_threads, sloppy_ordering, drop_final_batch)\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m       \u001b[0mprefetch_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_num_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_num_threads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m       sloppy_ordering, drop_final_batch))\n\u001b[0m\u001b[1;32m    891\u001b[0m make_batched_features_dataset_v2.__doc__ = (\n\u001b[1;32m    892\u001b[0m     make_batched_features_dataset_v1.__doc__)\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36mmake_batched_features_dataset_v2\u001b[0;34m(file_pattern, batch_size, features, reader, label_key, reader_args, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, reader_num_threads, parser_num_threads, sloppy_ordering, drop_final_batch)\u001b[0m\n\u001b[1;32m    855\u001b[0m   dataset = dataset.apply(\n\u001b[1;32m    856\u001b[0m       parsing_ops.parse_example_dataset(\n\u001b[0;32m--> 857\u001b[0;31m           features, num_parallel_calls=parser_num_threads))\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, transformation_func)\u001b[0m\n\u001b[1;32m   1851\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformation_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, transformation_func)\u001b[0m\n\u001b[1;32m   1288\u001b[0m           \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \"\"\"\n\u001b[0;32m-> 1290\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_apply_fn\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_apply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m\"\"\"Function from `Dataset` to `Dataset` that applies the transformation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mout_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ParseExampleDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     if any(\n\u001b[1;32m    134\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsing_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseFeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, features, num_parallel_calls)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_parallel_calls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsing_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepend_none_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# sparse_keys and dense_keys come back sorted here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     (sparse_keys, sparse_types, dense_keys, dense_types, dense_defaults,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_prepend_none_dimension\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prepend_none_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mmodified_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a copy to modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFixedLenSequenceFeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "d = Datasets.examples_via_feature_spec(os.path.join(training_dir, \"part-*\"),\n",
    "                transformed_feature_spec,\n",
    "                batch_size=25,\n",
    "                num_epochs=1000,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(file_pattern):\n",
    "        files = tf.data.Dataset.list_files(file_pattern=file_pattern) \n",
    "        files.apply(\n",
    "        tf.data.experimental.parallel_interleave(\n",
    "            lambda filename: tf.data.TFRecordDataset(filename),\n",
    "            cycle_length=32,\n",
    "            block_length=1,\n",
    "            sloppy=True,\n",
    "        ))\n",
    "        dataset = dataset.apply(tf.data.experimental.map_and_batch(\n",
    "        map_func=parse_record, batch_size=BATCH_SIZE, drop_remainder=False,\n",
    "        num_parallel_batches=1))\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pattern = 'gs://mo_ml/lingh/preprocessing/pdx_mo.Push.os_level_unsub.PreprocessingV1.OneHotFeatures/2019-08-21/20190827T190136.586152-1a00748f68d8/*.tfrecords'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'dataset' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9fd34bc0ac6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-88d8128c4515>\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m(file_pattern)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msloppy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         ))\n\u001b[0;32m---> 10\u001b[0;31m         dataset = dataset.apply(tf.data.experimental.map_and_batch(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         num_parallel_batches=1))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dataset' referenced before assignment"
     ]
    }
   ],
   "source": [
    "        def train_input_fn():\n",
    "            dataset = Datasets.examples_via_feature_spec(\n",
    "                os.path.join(training_dir, \"part-*\"),\n",
    "                feature_spec,\n",
    "                batch_size=25,\n",
    "                num_epochs=1000,\n",
    "                shuffle=True\n",
    "            )\n",
    "            return dataset.map(split_features_label_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mo_ml/lingh/tfrecords/pdx_mo.Push.BaseInputDataV1.train/2019-08-21/20190827T173336.732193-c52ea29f77ca/part-*'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(training_dir, \"part-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Dataset.range(1, 4)\n",
    "b = Dataset.range(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(x):\n",
    "    return tf.math.equal(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filter(filter_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Datasets' has no attribute '_get_featran_example_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3b1c1bea7faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_featran_example_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Datasets' has no attribute '_get_featran_example_dataset'"
     ]
    }
   ],
   "source": [
    "dataset, c = Datasets._get_featran_example_dataset(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.data.TFRecordDataset(input_file)\n",
    "d = d.shard(num_workers, worker_index)\n",
    "d = d.repeat(num_epochs)\n",
    "d = d.shuffle(shuffle_buffer_size)\n",
    "d = d.map(parser_fn, num_parallel_calls=num_map_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'paradox-mo'                            # {YOUR PROJECT ID}\n",
    "BUCKET = 'mo_ml'                     # {OUR BUCKET NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Python code\n",
    "MODEL_NAME = 'mo'                           # Model Info\n",
    "MODEL_VERSION = 'v1'                              # Model Version\n",
    "TRAINING_DIR = 'lingh/push/training_data'                     # Training Directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
    "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
    "\n",
    "axes = [0, 5, 0, 2]\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = per_clf.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.01002004, 0.        ],\n",
       "       [0.02004008, 0.        ],\n",
       "       ...,\n",
       "       [4.97995992, 2.        ],\n",
       "       [4.98997996, 2.        ],\n",
       "       [5.        , 2.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'paradox-mo'                            # {YOUR PROJECT ID}\n",
    "BUCKET = 'mo_ml'                     # {OUR BUCKET NAME}\n",
    "REGION = 'us-east1'                           # {YOUR BUCKET REGION} e.g. us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Python code\n",
    "MODEL_NAME = 'mo'                           # Model Info\n",
    "MODEL_VERSION = 'v1'                              # Model Version\n",
    "TRAINING_DIR = 'lingh/push/training_data'                     # Training Directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables (for bash code)\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "os.environ['MODEL_VERSION'] = MODEL_VERSION\n",
    "os.environ['TRAINING_DIR'] = TRAINING_DIR \n",
    "os.environ['TFVERSION'] = '1.13.1'                # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /Users/lingh/Git/ML/ai-platform-example\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Working Directory: ${PWD}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotify_tensorflow.dataset import Datasets\n",
    "from spotify_tensorflow.tf_schema_utils import schema_txt_file_to_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0 (default, Aug 21 2019, 18:31:23) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "1.14.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Examine software versions\n",
    "print(__import__('sys').version)\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the absolutte paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pattern = 'gs://mo_ml/lingh/tfrecords/pdx_mo.Push.BaseInputDataV1.train/2019-08-21/20190827T173336.732193-c52ea29f77ca/*.tfrecords'\n",
    "filenames = tf.train.match_filenames_once(filename_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 14:33:38.022505 4602758592 deprecation.py:323] From <ipython-input-8-006592a6f525>:2: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0903 14:33:38.028455 4602758592 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0903 14:33:38.030497 4602758592 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "W0903 14:33:38.034978 4602758592 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/training/input.py:113: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "W0903 14:33:38.035858 4602758592 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:2322: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "W0903 14:33:38.039554 4602758592 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0903 14:33:38.041412 4602758592 deprecation.py:323] From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(\n",
    "   filenames, num_epochs=100, shuffle=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "_, serialized = reader.read_up_to(filename_queue, num_records=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 14:38:45.751729 4602758592 deprecation.py:323] From <ipython-input-14-96ffa3882748>:11: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_threads = 100\n",
    "num_threads_cnt = 2\n",
    "raw_features = tf.train.shuffle_batch(\n",
    "   [serialized],\n",
    "   batch_size,\n",
    "   min_after_dequeue=2 * batch_size + 1,\n",
    "   capacity=batch_size * 10,\n",
    "   num_threads=num_threads_cnt,\n",
    "   enqueue_many=True,\n",
    "   allow_smaller_final_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7114ba9e7a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_examples' is not defined"
     ]
    }
   ],
   "source": [
    "features, labels = parse_examples(raw_features)\n",
    "return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"gs://mo_ml/lingh/push/training_data\"\n",
    "for root, dirs, files in os.walk(name):\n",
    "    indent = '    ' * root.count(os.sep)\n",
    "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "    for filename in files:\n",
    "        print('{}{}'.format(indent + '    ', filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object walk at 0x1080ec410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.walk(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.io import Read, tfrecordio\n",
    "from apache_beam.io.filesystem import CompressionTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "No files found based on the file pattern gs://mo_ml/lingh/push/training_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f413097065a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gs://mo_ml/lingh/push/training_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfrecordio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadFromTFRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCompressionTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_pattern, coder, compression_type, validate, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReadFromTFRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     self._source = _TFRecordSource(file_pattern, coder, compression_type,\n\u001b[0;32m--> 262\u001b[0;31m                                    validate)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_pattern, coder, compression_type, validate)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0msplittable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         validate=validate)\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/apache_beam/io/filebasedsource.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, splittable, validate)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_splittable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfile_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_accessible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/apache_beam/options/value_provider.pyc\u001b[0m in \u001b[0;36m_f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_accessible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRuntimeValueProviderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not accessible'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfnc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_check_accessible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/apache_beam/io/filebasedsource.pyc\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       raise IOError(\n\u001b[0;32m--> 181\u001b[0;31m           'No files found based on the file pattern %s' % pattern)\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   def split(\n",
      "\u001b[0;31mIOError\u001b[0m: No files found based on the file pattern gs://mo_ml/lingh/push/training_data"
     ]
    }
   ],
   "source": [
    "input_data_dir = \"gs://mo_ml/lingh/push/training_data\"\n",
    "training = tfrecordio.ReadFromTFRecord(input_data_dir, compression_type=CompressionTypes.AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ghe.spotify.net/bart/bart-transform/blob/941a5e346138c3d77db37b9cb7a7b931a6198957/tft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
    "  \"\"\"Generates an input function to be used for model training.\n",
    "\n",
    "  Args:\n",
    "    features: numpy array of features used for training or inference\n",
    "    labels: numpy array of labels for each example\n",
    "    shuffle: boolean for whether to shuffle the data or not (set True for\n",
    "      training, False for evaluation)\n",
    "    num_epochs: number of epochs to provide the data for\n",
    "    batch_size: batch size for training\n",
    "\n",
    "  Returns:\n",
    "    A tf.data.Dataset that can provide data to the Keras model for training or\n",
    "      evaluation\n",
    "  \"\"\"\n",
    "  if labels is None:\n",
    "    inputs = features\n",
    "  else:\n",
    "    inputs = (features, labels)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # load data from gcs\n",
    "    gcs_specs = {\n",
    "        \"gcs_path\": \"gs://{}/tf-records-iris/{}/train\".format(GCS_BUCKET, partition),\n",
    "        \"file_pattern\": \"part-*\",\n",
    "        \"schema_file_name\": \"_inferred_schema.pb\",\n",
    "        \"featran_settings_path\":\n",
    "            \"gs://{}/tf-records-iris/{}/settings\".format(GCS_BUCKET, partition),\n",
    "        \"label_key\": \"class_name\"\n",
    "    }\n",
    "    model.load_data(source='gcs-tfrecords',\n",
    "                    specs=gcs_specs,\n",
    "                    perc_val_split=0.3,\n",
    "                    stratified_split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_path = pjoin(os.path.dirname(os.path.realpath(__file__)), \"../trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmd():\n",
    "        output_uri = pjoin(self.get_hades_target_uri()[\"output\"], \"export\")\n",
    "        cmd = (\"python ./python/trainer/train.py \"\n",
    "               \"--tf-transform-dir {input} \"\n",
    "               \"--job-dir {output} \").format(input=self.input().uri(),\n",
    "                                             output=output_uri)\n",
    "        for (k, v) in self.trial_parameters().items():\n",
    "            cmd += \"--{k}={v} \".format(k=k, v=v)\n",
    "        return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cmd().output_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
