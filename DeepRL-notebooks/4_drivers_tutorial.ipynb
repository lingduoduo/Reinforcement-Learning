{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already up-to-date: tensorflow-probability in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=1.2.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability) (1.14.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already satisfied: tf-agents in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (1.14.0)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.1.3)\n",
      "Requirement already satisfied: tensorflow-probability>=0.6.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tf-agents) (1.18.1)\n",
      "Requirement already satisfied: gast>=0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (0.3.3)\n",
      "Requirement already satisfied: decorator in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (4.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from tensorflow-probability>=0.6.0->tf-agents) (1.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already satisfied: gym in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (0.10.11)\n",
      "Requirement already satisfied: six in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym) (1.18.1)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: future in /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "# Note: If you haven't installed tf-agents or gym yet, run:\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "!pip install --upgrade tensorflow-probability\n",
    "!pip install tf-agents\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyDriver(object):\n",
    "\n",
    "    def __init__(self, env, policy, observers, max_steps=1, max_episodes=1):\n",
    "        self._env = env\n",
    "        self._policy = policy\n",
    "        self._observers = observers or []\n",
    "        self._max_steps = max_steps or np.inf\n",
    "        self._max_episodes = max_episodes or np.inf\n",
    "\n",
    "    def run(self, time_step, policy_state=()):\n",
    "        num_steps = 0\n",
    "        num_episodes = 0\n",
    "        while num_steps < self._max_steps and num_episodes < self._max_episodes:\n",
    "\n",
    "            # Compute an action using the policy for the given time_step\n",
    "            action_step = self._policy.action(time_step, policy_state)\n",
    "\n",
    "            # Apply the action to the environment and get the next step\n",
    "            next_time_step = self._env.step(action_step.action)\n",
    "\n",
    "            # Package information into a trajectory\n",
    "            traj = trajectory.Trajectory(\n",
    "             time_step.step_type,\n",
    "             time_step.observation,\n",
    "             action_step.action,\n",
    "             action_step.info,\n",
    "             next_time_step.step_type,\n",
    "             next_time_step.reward,\n",
    "             next_time_step.discount)\n",
    "\n",
    "            for observer in self._observers:\n",
    "                observer(traj)\n",
    "\n",
    "            # Update statistics to check termination\n",
    "            num_episodes += np.sum(traj.is_last())\n",
    "            num_steps += np.sum(~traj.is_boundary())\n",
    "\n",
    "            time_step = next_time_step\n",
    "            policy_state = action_step.state\n",
    "\n",
    "        return time_step, policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer:\n",
      "Trajectory(step_type=array(0, dtype=int32), observation=array([ 0.03968893,  0.02144465, -0.0392565 , -0.0027633 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.04011782, -0.17309296, -0.03931177,  0.27727985], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03665596,  0.02256713, -0.03376617, -0.02753823], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.0371073 , -0.17205472, -0.03431693,  0.25430277], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.03366621, -0.3666703 , -0.02923088,  0.5359671 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.0263328 , -0.5613693 , -0.01851154,  0.81929815], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([ 0.01510542, -0.7562331 , -0.00212557,  1.1061016 ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-1.9244029e-05, -5.6108326e-01,  1.9996461e-02,  8.1275266e-01],\n",
      "      dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.01124091, -0.7564733 ,  0.03625152,  1.1116577 ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.02637037, -0.5618458 ,  0.05848467,  0.83056384], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.03760729, -0.36756992,  0.07509594,  0.55683297], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.04495869, -0.17357811,  0.0862326 ,  0.28872284], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.04843025, -0.36981714,  0.09200706,  0.6073086 ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.05582659, -0.17609385,  0.10415323,  0.34496468], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.05934847, -0.3725313 ,  0.11105253,  0.66858983], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.0667991 , -0.56900793,  0.12442432,  0.9940717 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.07817926, -0.76555485,  0.14430577,  1.3230993 ], dtype=float32), action=array(1), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.09349035, -0.5725203 ,  0.17076774,  1.078836  ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32))\n",
      "Trajectory(step_type=array(1, dtype=int32), observation=array([-0.10494076, -0.7694349 ,  0.19234446,  1.4198736 ], dtype=float32), action=array(0), policy_info=(), next_step_type=array(2, dtype=int32), reward=array(1., dtype=float32), discount=array(0., dtype=float32))\n",
      "Average Return:  19.0\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "policy = random_py_policy.RandomPyPolicy(time_step_spec=env.time_step_spec(), \n",
    "                                         action_spec=env.action_spec())\n",
    "replay_buffer = []\n",
    "metric = py_metrics.AverageReturnMetric()\n",
    "observers = [replay_buffer.append, metric]\n",
    "driver = py_driver.PyDriver(env, policy, observers, max_steps=20, max_episodes=1)\n",
    "\n",
    "initial_time_step = env.reset()\n",
    "final_time_step, _ = driver.run(initial_time_step)\n",
    "\n",
    "print('Replay Buffer:')\n",
    "for traj in replay_buffer:\n",
    "  print(traj)\n",
    "\n",
    "print('Average Return: ', metric.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lingh/.pyenv/versions/3.7.0/envs/my-virtual-env-3.7.0/lib/python3.7/site-packages/tf_agents/drivers/dynamic_episode_driver.py:221: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
      "final_time_step TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.01141971,  0.03352176,  0.027735  , -0.02424971]],\n",
      "      dtype=float32)>)\n",
      "Number of Steps:  68\n",
      "Number of Episodes:  2\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
    "                                            time_step_spec=tf_env.time_step_spec())\n",
    "\n",
    "\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=2)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_time_step TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[-0.02745439, -0.02013117,  0.0220114 , -0.00767116]],\n",
      "      dtype=float32)>)\n",
      "Number of Steps:  107\n",
      "Number of Episodes:  4\n"
     ]
    }
   ],
   "source": [
    "# Continue running from previous state\n",
    "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
